# Pattern 3: N/S-Caching Scale-Out Deployment on NVIDIA T4 GPUs
#
# This configuration deploys Qwen2.5-3B-Instruct across 3 GPU replicas with:
# - vLLM prefix caching enabled for efficient repeated prompt handling
# - Intelligent routing based on prefix cache affinity and KV cache utilization
# - NVIDIA T4 GPU acceleration (1 GPU per replica)
#
# Key Differences from Pattern 1:
# - 3 replicas instead of 1 (scale-out for throughput)
# - Prefix caching enabled (--enable-prefix-caching)
# - Reduced GPU memory utilization (0.80 vs 0.85) to accommodate cache overhead
# - Intelligent routing with prefix-cache-scorer (weight 3.0)
#
# Key Differences from TPU Pattern 3:
# - Image: ghcr.io/llm-d/llm-d-cuda:v0.4.0 (GPU) vs vllm/vllm-tpu:v0.11.1 (TPU)
# - Resource: nvidia.com/gpu: 1 (single GPU) vs google.com/tpu: 4 (4-chip TPU)
# - No tensor parallelism (single GPU) vs TP=4 (required for TPU)
# - GPU memory management (--gpu-memory-utilization) vs HBM (managed by JAX)
# - Faster startup (3-5 min CUDA) vs longer (5-7 min XLA precompilation)

modelArtifacts:
  uri: "hf://Qwen/Qwen2.5-3B-Instruct"
  name: "Qwen/Qwen2.5-3B-Instruct"
  size: 15Gi
  authSecretName: "huggingface-token"

accelerator:
  type: nvidia

routing:
  proxy:
    enabled: false
    targetPort: 8000

decode:
  create: true
  replicas: 3  # KEY: 3 replicas for horizontal scale-out

  monitoring:
    podmonitor:
      enabled: true
      portName: "vllm"
      path: "/metrics"
      interval: "30s"

  containers:
  - name: "vllm"
    image: ghcr.io/llm-d/llm-d-cuda:v0.4.0
    modelCommand: vllmServe
    args:
      - "--disable-uvicorn-access-log"
      - "--max-model-len=2048"
      - "--gpu-memory-utilization=0.75"  # Reduced from 0.80 to avoid OOM during sampler warmup
      - "--enable-prefix-caching"  # NEW: Enable vLLM prefix caching for efficient repeated prompts
    ports:
      - containerPort: 8000
        name: vllm
        protocol: TCP

    resources:
      limits:
        nvidia.com/gpu: "1"
      requests:
        nvidia.com/gpu: "1"

    mountModelVolume: true
    volumeMounts:
    - name: metrics-volume
      mountPath: /.config
    - name: torch-compile-cache
      mountPath: /.cache

    startupProbe:
      httpGet:
        path: /v1/models
        port: vllm
      initialDelaySeconds: 30
      periodSeconds: 30
      timeoutSeconds: 5
      failureThreshold: 60  # Allow 30 minutes for model download + CUDA compilation

    livenessProbe:
      httpGet:
        path: /health
        port: vllm
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3

    readinessProbe:
      httpGet:
        path: /v1/models
        port: vllm
      periodSeconds: 5
      timeoutSeconds: 2
      failureThreshold: 3

  volumes:
  - name: metrics-volume
    emptyDir: {}
  - name: torch-compile-cache
    emptyDir: {}

prefill:
  create: false

multinode: false
