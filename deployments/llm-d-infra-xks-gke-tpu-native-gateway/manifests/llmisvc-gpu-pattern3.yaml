apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: qwen2-3b-pattern3-gpu
  namespace: llm-d-inference-scheduling
spec:
  model:
    uri: hf://Qwen/Qwen2.5-3B-Instruct
    name: Qwen/Qwen2.5-3B-Instruct
  replicas: 3  # Pattern 3: scale-out with 3 replicas for high throughput

  # Router configuration
  # KServe controller auto-creates HTTPRoute and InferencePool with EPP scheduler
  # Uses GKE regional Gateway (gke-l7-regional-external-managed) for InferencePool support
  # EPP scheduler provides intelligent routing with prefix-cache awareness
  router:
    route: {}      # Auto-create HTTPRoute
    gateway:
      refs:
      - name: inference-gateway
        namespace: opendatahub
    scheduler: {}  # Enable EPP scheduler for intelligent routing with prefix cache awareness

  # vLLM container template
  template:
    # GPU node selection
    nodeSelector:
      cloud.google.com/gke-accelerator: nvidia-tesla-t4

    # Tolerate GPU node pool taint (if configured)
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

    containers:
    - name: main
      image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.0.0

      # vLLM command (KServe wraps this with /bin/bash -c)
      # Model downloaded by init container to /mnt/models
      # vLLM serves HTTP directly (no TLS sidecars)
      # Pattern 3 additions:
      #   --enable-prefix-caching: Enable KV cache prefix sharing
      #   --prefix-cache-block-size=16: Cache granularity (16 tokens per block)
      #   --gpu-memory-utilization=0.75: Reduced from 0.85 for sampler warmup (Pattern 3 recommendation)
      args:
      - |
        python3 -m vllm.entrypoints.openai.api_server \
          --model=/mnt/models \
          --dtype=half \
          --max-model-len=2048 \
          --gpu-memory-utilization=0.75 \
          --enable-prefix-caching \
          --prefix-cache-block-size=16 \
          --disable-log-requests

      # GPU environment variables
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: hf-token
            key: HF_TOKEN

      # Resource allocation
      # Pattern 3 requires 3 nodes Ã— 1 GPU each = 3 total GPUs
      resources:
        limits:
          nvidia.com/gpu: "1"  # Per replica: 1 GPU
        requests:
          nvidia.com/gpu: "1"

      # Health probes (HTTP - no sidecars, no service mesh)
      # Extended delays for GPU initialization + model download + CUDA compilation
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 180  # GPU init (1-2 min) + model download
        periodSeconds: 30
        timeoutSeconds: 30
        failureThreshold: 5

      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 180
        periodSeconds: 10
        timeoutSeconds: 10

    # Pull secret (must be copied to namespace from cert-manager)
    imagePullSecrets:
    - name: redhat-pull-secret
